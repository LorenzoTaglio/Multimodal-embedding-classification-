{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81b67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import f1_score, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe114800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"..\\\\data\\\\processed\\\\df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"..\\\\data\\\\processed\\\\df_test.pkl\")\n",
    "\n",
    "text_embeddings_train = torch.load(\"..\\\\data\\\\processed\\\\rocov2_captions_embeddings_train.pt\")\n",
    "text_embeddings_test = torch.load(\"..\\\\data\\\\processed\\\\rocov2_captions_embeddings_test.pt\")\n",
    "\n",
    "image_embeddings_train = torch.load(\"..\\\\data\\\\processed\\\\rocov2_image_embeddings_train.pt\")\n",
    "image_embeddings_test = torch.load(\"..\\\\data\\\\processed\\\\rocov2_image_embeddings_test.pt\")\n",
    "\n",
    "# optiimzation for better memory management\n",
    "text_embeddings_train = text_embeddings_train.detach().cpu().numpy().astype(\"float32\")\n",
    "text_embeddings_test = text_embeddings_test.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "image_embeddings_train = image_embeddings_train.detach().cpu().numpy().astype(\"float32\")\n",
    "image_embeddings_test = image_embeddings_test.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "# get merged embeddings\n",
    "combined_embeddings_train = np.concatenate((text_embeddings_train, image_embeddings_train), axis=1)\n",
    "combined_embeddings_test = np.concatenate((text_embeddings_test, image_embeddings_test), axis=1)\n",
    "\n",
    "combined_embeddings_train = combined_embeddings_train.astype(\"float32\")\n",
    "combined_embeddings_test = combined_embeddings_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pca and mlb\n",
    "pca = PCA(n_components=300, random_state=42)\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf92c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels\n",
    "y_train = mlb.fit_transform(df_train['Semantic_vec'])\n",
    "y_test = mlb.transform(df_test['Semantic_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7e34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "X_train = pca.fit_transform(combined_embeddings_train)\n",
    "X_test = pca.transform(combined_embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc0c9b",
   "metadata": {},
   "source": [
    "# Ensemble of classifier chains with random undersampling\n",
    "This technique is based off this paper: https://arxiv.org/pdf/1807.11393 <br>\n",
    "The algorithm shown was then optimized with parallelization of the ClassifierChains and batch testing of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d288d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CCRU(X, y, CH):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    Xj = X.copy()\n",
    "    h = {}\n",
    "    \n",
    "    for j_idx, j in enumerate(CH):  # Use j_idx for iteration, j for actual label index\n",
    "        \n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_us, y_us = rus.fit_resample(Xj, y[:, j])  # Use j (actual label index)\n",
    "        \n",
    "        \n",
    "        lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "        lr.fit(X_us, y_us)  \n",
    "        h[j] = lr\n",
    "        \n",
    "        # Add true label as feature for next classifier (except for the last one)\n",
    "        if j_idx < len(CH) - 1:  # Use j_idx for comparison\n",
    "            y_true = y[:, j]  # Use j (actual label index) to get the true labels\n",
    "            Xj = sparse.hstack([X_us, y_true.reshape(-1, 1)]).tocsr()\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae602d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_replacement(X,y):\n",
    "    sample_size = len(X)\n",
    "\n",
    "    sampled_indices = random.choices(range(sample_size), k=sample_size)\n",
    "    \n",
    "    X_np = np.array(X)\n",
    "    y_np = np.array(y)\n",
    "\n",
    "    X_prime = X_np[sampled_indices]\n",
    "    y_prime = y_np[sampled_indices]\n",
    "\n",
    "    return X_prime, y_prime\n",
    "\n",
    "\n",
    "def get_minority(y, pos):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    \n",
    "    class_counts = np.bincount(y[:, pos])\n",
    "    return np.min(class_counts)\n",
    "\n",
    "def get_cj(y, c, j, q):\n",
    "    # Ensure y is a numpy array\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    \n",
    "    # Calculate the sum of minority counts for all labels\n",
    "    sum_of_minorities = 0\n",
    "    for k in range(q):\n",
    "        counts = np.bincount(y[:, k])\n",
    "        sum_of_minorities += np.min(counts[np.nonzero(counts)])\n",
    "\n",
    "    # Calculate the minority count for the specific label j\n",
    "    counts_j = np.bincount(y[:, j])\n",
    "    minority_j = np.min(counts_j[np.nonzero(counts_j)])\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if minority_j == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return (c * sum_of_minorities) / (q * minority_j)\n",
    "\n",
    "\n",
    "\n",
    "def train_ECCRU_optimized(X, y, labels_num, standard_chains_num, coefficient):\n",
    "    # 1. Pre-generate all chain permutations and bootstrap indices.\n",
    "    #    This is done once, upfront.\n",
    "\n",
    "    c_omega_max = int(standard_chains_num * coefficient)\n",
    "\n",
    "    classifiers_for_each_label = [min(get_cj(y, standard_chains_num, j, labels_num), c_omega_max) for j in range(labels_num)]\n",
    "\n",
    "    permutations = []\n",
    "    for i in range(c_omega_max):\n",
    "        S = [j for j in range(labels_num) if classifiers_for_each_label[j] > 0]\n",
    "        if len(S) < 2:\n",
    "            break\n",
    "        CH = np.random.permutation(S).tolist()\n",
    "        permutations.append(CH)\n",
    "        for j in S:\n",
    "            classifiers_for_each_label[j] -= 1\n",
    "\n",
    "    # 2. Define a training function that takes the full dataset and bootstrap indices.\n",
    "    #    This allows the function to be called in parallel.\n",
    "    def train_single_chain_optimized(X_full, y_full, sampled_indices, CH):\n",
    "        X_sampled = X_full[sampled_indices]\n",
    "        y_sampled = y_full[sampled_indices]\n",
    "        return train_CCRU(X=X_sampled, y=y_sampled,CH=CH)\n",
    "\n",
    "    # 3. Use Parallel to execute the training. The large arrays (X_full, y_full)\n",
    "    #    are passed to each process, but Python's multiprocessing will often\n",
    "    #    use shared memory for this, which is more efficient. The key is to\n",
    "    #    pass only the indices to `train_single_chain_optimized`.\n",
    "\n",
    "    # Pre-generate bootstrap indices for each chain\n",
    "    sample_size = len(X)\n",
    "    bootstrap_indices = [np.random.choice(sample_size, size=sample_size, replace=True) for _ in range(len(permutations))]\n",
    "\n",
    "    print(f\"Numbers of classifier chains: {len(permutations)}\")\n",
    "\n",
    "    H = Parallel(n_jobs=14, verbose=10)(\n",
    "        delayed(train_single_chain_optimized)(X, y, indices, CH)\n",
    "        for indices, CH in tzip(bootstrap_indices, permutations)\n",
    "    )\n",
    "\n",
    "    counter = len(H)\n",
    "    return H, permutations, counter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af3ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ECCRU_optimized(X, labels_num, eccru, permutations, cc_num=None, threshold=0.5):\n",
    "    X_np = np.array(X)\n",
    "    \n",
    "    # Sanity Check: Confirm the input is a 2D array\n",
    "    if X_np.ndim != 2:\n",
    "        raise ValueError(f\"Input to test_ECCRU_optimized must be a 2D array, but got shape {X_np.shape}\")\n",
    "    \n",
    "    num_samples = X_np.shape[0]\n",
    "    \n",
    "    sum_of_probabilities_for_batch = np.zeros((num_samples, labels_num), dtype=float)\n",
    "    label_prediction_counts = np.zeros(labels_num, dtype=int)\n",
    "\n",
    "    num_chains_to_use = len(eccru) if cc_num is None else cc_num\n",
    "    if num_chains_to_use == 0:\n",
    "        return np.zeros((num_samples, labels_num), dtype=int)\n",
    "\n",
    "    for i in range(num_chains_to_use):\n",
    "        if i >= len(eccru):\n",
    "            break\n",
    "\n",
    "        chain_h_dict = eccru[i]\n",
    "        CH_sequence = permutations[i]\n",
    "        current_X_for_chain = X_np.copy()\n",
    "\n",
    "        for j_idx_in_chain in range(len(CH_sequence)):\n",
    "            actual_label_index = CH_sequence[j_idx_in_chain]  # This is the actual label index\n",
    "            \n",
    "            # Use j_idx_in_chain to get the correct model from the chain\n",
    "            model_for_label = chain_h_dict[j_idx_in_chain]\n",
    "            \n",
    "            predicted_probs_for_label_all_samples = model_for_label.predict_proba(current_X_for_chain)[:, 1]\n",
    "\n",
    "            # Use actual_label_index to accumulate predictions for the correct label\n",
    "            sum_of_probabilities_for_batch[:, actual_label_index] += predicted_probs_for_label_all_samples\n",
    "            label_prediction_counts[actual_label_index] += 1\n",
    "            \n",
    "            binary_predictions_for_label_all_samples = (predicted_probs_for_label_all_samples >= 0.5).astype(int)\n",
    "\n",
    "            # Add predictions as features for next classifier (except for the last one)\n",
    "            if j_idx_in_chain < len(CH_sequence) - 1:\n",
    "                current_X_for_chain = np.hstack([current_X_for_chain, binary_predictions_for_label_all_samples.reshape(-1, 1)])\n",
    "\n",
    "    final_relevance_degrees_batch = np.divide(\n",
    "        sum_of_probabilities_for_batch,\n",
    "        label_prediction_counts,\n",
    "        out=np.zeros_like(sum_of_probabilities_for_batch, dtype=float),\n",
    "        where=label_prediction_counts != 0\n",
    "    )\n",
    "\n",
    "    final_binary_predictions = (final_relevance_degrees_batch >= threshold).astype(int)\n",
    "    return final_binary_predictions\n",
    "\n",
    "def test_on_chunk(X_chunk, labels_num, eccru, permutations, cc_num):\n",
    "    \"\"\"\n",
    "    Wrapper to ensure the correct arguments are passed.\n",
    "    \"\"\"\n",
    "    return test_ECCRU_optimized(X_chunk, labels_num, eccru, permutations, cc_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed85aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_chunks(X, n_jobs):\n",
    "    \"\"\"\n",
    "    Crea chunks bilanciati che non perdono campioni\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    chunk_size = n_samples // n_jobs\n",
    "    chunks = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for i in range(n_jobs):\n",
    "        if i == n_jobs - 1:  # Ultimo chunk prende tutti i campioni rimanenti\n",
    "            end_idx = n_samples\n",
    "        else:\n",
    "            end_idx = start_idx + chunk_size\n",
    "        \n",
    "        chunks.append(X[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def safe_parallel_processing(X_test, labels_num, eccru, permutations, counter, n_jobs=None):\n",
    "    \"\"\"\n",
    "    Processing parallelo che garantisce di non perdere campioni\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = os.cpu_count()\n",
    "    \n",
    "    n_samples = len(X_test)\n",
    "    print(f\"Campioni totali da processare: {n_samples}\")\n",
    "    \n",
    "    # Crea chunks bilanciati\n",
    "    chunks = create_balanced_chunks(X_test, n_jobs)\n",
    "    \n",
    "    print(f\"Creati {len(chunks)} chunks con dimensioni: {[len(c) for c in chunks]}\")\n",
    "    \n",
    "    # Processing parallelo\n",
    "    y_preds_chunks = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=1)(\n",
    "        delayed(test_on_chunk)(chunk, labels_num, eccru, permutations, counter)\n",
    "        for chunk in chunks\n",
    "    )\n",
    "    \n",
    "    # Concatena i risultati\n",
    "    y_preds = np.concatenate(y_preds_chunks, axis=0)\n",
    "    \n",
    "    print(f\"Risultato finale: {y_preds.shape}\")\n",
    "    print(f\"Match con input: {len(y_preds) == n_samples}\")\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "def fix_existing_predictions(y_preds, target_length):\n",
    "    \"\"\"\n",
    "    Corregge predizioni esistenti per matchare la lunghezza target\n",
    "    \"\"\"\n",
    "    current_length = len(y_preds)\n",
    "    print(f\"Lunghezza attuale predizioni: {current_length}\")\n",
    "    print(f\"Lunghezza target: {target_length}\")\n",
    "    \n",
    "    if current_length > target_length:\n",
    "        print(f\"Troncando {current_length - target_length} campioni extra\")\n",
    "        return y_preds[:target_length]\n",
    "    elif current_length < target_length:\n",
    "        print(f\"ERRORE: Mancano {target_length - current_length} campioni!\")\n",
    "        print(\"Dovrai rifare il processing.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Le dimensioni sono giÃ  corrette\")\n",
    "        return y_preds\n",
    "\n",
    "def complete_eccru_evaluation(X_test, y_test: None, df_test: None, mlb, eccru, permutations, counter):\n",
    "    \"\"\"\n",
    "    Valutazione completa ECCRU con controlli di sicurezza\n",
    "    \"\"\"\n",
    "    print(\"=== INIZIO VALUTAZIONE ECCRU ===\")\n",
    "    \n",
    "    # 1. Verifica dimensioni iniziali\n",
    "    # print(f\"X_test shape: {X_test.shape}\")\n",
    "    # print(f\"df_test shape: {df_test.shape}\")\n",
    "    \n",
    "    # 2. Crea y_test di riferimento\n",
    "    if y_test is None:\n",
    "        if df_test is not None and mlb is not None:\n",
    "            y_test = mlb.transform(df_test[\"Semantic_vec\"])\n",
    "            print(f\"y_test shape: {y_test.shape}\")\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # 3. Processing sicuro\n",
    "    y_preds = safe_parallel_processing(X_test, 16, eccru, permutations, counter)\n",
    "    \n",
    "    # 4. Verifica finale delle dimensioni\n",
    "    if len(y_preds) != len(y_test):\n",
    "        print(f\"ERRORE: Mismatch dimensioni!\")\n",
    "        print(f\"y_preds: {len(y_preds)}, y_test: {len(y_test)}\")\n",
    "        \n",
    "        # Tenta correzione\n",
    "        y_preds = fix_existing_predictions(y_preds, len(y_test))\n",
    "        if y_preds is None:\n",
    "            return None\n",
    "    \n",
    "    # 5. Calcola metriche\n",
    "    y_preds_binary = (np.array(y_preds) >= 0.5).astype(int)\n",
    "    \n",
    "    # f1_micro = f1_score(y_test, y_preds_binary, average='micro')\n",
    "    # f1_macro = f1_score(y_test, y_preds_binary, average='macro')\n",
    "    # f1_samples = f1_score(y_test, y_preds_binary, average=\"samples\")\n",
    "    # hamming = hamming_loss(y_test, y_preds_binary)\n",
    "    \n",
    "    # 6. Risultati\n",
    "    # print(\"\\n=== RISULTATI FINALI ===\")\n",
    "    # print(f\"F1 micro: {f1_micro:.4f}\")\n",
    "    # print(f\"F1 macro: {f1_macro:.4f}\")\n",
    "    # print(f\"F1 samples: {f1_samples:.4f}\")\n",
    "    # print(f\"Hamming loss: {hamming:.4f}\")\n",
    "    \n",
    "    # return {\n",
    "    #     'y_preds': y_preds_binary,\n",
    "    #     'y_test': y_test,\n",
    "    #     'metrics': {\n",
    "    #         'f1_micro': f1_micro,\n",
    "    #         'f1_macro': f1_macro, \n",
    "    #         'f1_samples': f1_samples,\n",
    "    #         'hamming_loss': hamming\n",
    "    #     }\n",
    "    # }\n",
    "    return y_preds_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of classifier chains: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858c05ea703d42d29bd1f7fa6b5baf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 59898, expected 250.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\loret\\AppData\\Local\\Temp\\ipykernel_7584\\3667357328.py\", line 68, in train_single_chain_optimized\n  File \"C:\\Users\\loret\\AppData\\Local\\Temp\\ipykernel_7584\\2696643583.py\", line 21, in train_CCRU\n  File \"d:\\Multimodal embedding\\myenv\\Lib\\site-packages\\scipy\\sparse\\_construct.py\", line 801, in hstack\n    return _block([blocks], format, dtype, return_spmatrix=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Multimodal embedding\\myenv\\Lib\\site-packages\\scipy\\sparse\\_construct.py\", line 1016, in _block\n    raise ValueError(msg)\nValueError: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 59898, expected 250.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ecc, permutations, counter = \u001b[43mtrain_ECCRU_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m results = complete_eccru_evaluation(X_test, y_test=y_test, df_test=\u001b[38;5;28;01mNone\u001b[39;00m, mlb=\u001b[38;5;28;01mNone\u001b[39;00m, eccru=ecc, permutations=permutations, counter=counter)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mf1-micro: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score(y_test,results,\u001b[38;5;250m \u001b[39maverage=\u001b[33m\"\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mf1-macro: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score(y_test,results,\u001b[38;5;250m \u001b[39maverage=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mf1-samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score(y_test,results,\u001b[38;5;250m \u001b[39maverage=\u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mhamming: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhamming_loss(y_test,results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mtrain_ECCRU_optimized\u001b[39m\u001b[34m(X, y, labels_num, standard_chains_num, coefficient)\u001b[39m\n\u001b[32m     77\u001b[39m bootstrap_indices = [np.random.choice(sample_size, size=sample_size, replace=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(permutations))]\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumbers of classifier chains: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(permutations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m H = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_single_chain_optimized\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtzip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbootstrap_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermutations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m counter = \u001b[38;5;28mlen\u001b[39m(H)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m H, permutations, counter\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Multimodal embedding\\myenv\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 59898, expected 250."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ecc, permutations, counter = train_ECCRU_optimized(X_train,y_train, 16, 10, 10)\n",
    "results = complete_eccru_evaluation(X_test, y_test=y_test, df_test=None, mlb=None, eccru=ecc, permutations=permutations, counter=counter)\n",
    "\n",
    "print(f\"f1-micro: {f1_score(y_test,results, average=\"micro\")}\\nf1-macro: {f1_score(y_test,results, average=\"macro\")}\\nf1-samples: {f1_score(y_test,results, average=\"samples\")}\\nhamming: {hamming_loss(y_test,results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
